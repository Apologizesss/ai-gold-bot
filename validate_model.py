"""
Model Validation Script
=======================
‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Model Overfit ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

Features:
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Train vs Test accuracy gap
- Cross-validation
- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Confusion Matrix
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Leakage

‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ:
    python validate_model.py
    python validate_model.py --model models/lstm_simple_20251105_122005.keras
"""

import pandas as pd
import numpy as np
from pathlib import Path
import pickle
import argparse
import sys
import warnings

warnings.filterwarnings("ignore")

# Deep Learning
from tensorflow import keras
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import xgboost as xgb

print("=" * 80)
print("üîç Model Validation Script")
print("=" * 80)


def load_lstm_model(model_path):
    """‡πÇ‡∏´‡∏•‡∏î LSTM model"""
    print(f"\nüìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î LSTM Model: {model_path}")

    model_path = Path(model_path)
    if not model_path.exists():
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö model: {model_path}")
        return None, None, None

    # ‡πÇ‡∏´‡∏•‡∏î model
    model = keras.models.load_model(model_path)

    # ‡∏´‡∏≤ scaler ‡πÅ‡∏•‡∏∞ features
    timestamp = model_path.stem.split("_")[-2] + "_" + model_path.stem.split("_")[-1]
    base_name = "_".join(model_path.stem.split("_")[:-2])

    scaler_path = model_path.parent / f"scaler_{base_name}_{timestamp}.pkl"
    features_path = model_path.parent / f"features_{base_name}_{timestamp}.pkl"

    scaler = None
    feature_cols = None

    if scaler_path.exists():
        with open(scaler_path, "rb") as f:
            scaler = pickle.load(f)
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î scaler: {scaler_path.name}")

    if features_path.exists():
        with open(features_path, "rb") as f:
            feature_cols = pickle.load(f)
        print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î features: {len(feature_cols)} features")

    return model, scaler, feature_cols


def load_xgboost_model():
    """‡πÇ‡∏´‡∏•‡∏î XGBoost model"""
    print(f"\nüìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î XGBoost Model")

    model_path = Path("results/xgboost/xgboost_model.pkl")
    scaler_path = Path("results/xgboost/xgboost_scaler.pkl")

    if not model_path.exists():
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö XGBoost model")
        return None, None

    # ‡πÇ‡∏´‡∏•‡∏î model
    with open(model_path, "rb") as f:
        model = pickle.load(f)

    # ‡πÇ‡∏´‡∏•‡∏î scaler
    scaler = None
    if scaler_path.exists():
        try:
            with open(scaler_path, "rb") as f:
                scaler = pickle.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î scaler: {e}")
            scaler = None

    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î XGBoost model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    return model, scaler


def load_data(data_path):
    """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    print(f"\nüìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {data_path}")

    if not Path(data_path).exists():
        print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {data_path}")
        return None, None, None

    df = pd.read_csv(data_path)
    print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(df):,} ‡πÅ‡∏ñ‡∏ß")

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    exclude_cols = [
        "target",
        "future_price",
        "future_high",
        "future_low",
        "max_gain_pct",
        "max_loss_pct",
        "max_gain",
        "max_loss",
        "threshold",
        "future_close",
        "future_return",
        "gain_pct",
        "loss_pct",
        "score",
        "time",
        "timestamp",
        "symbol",
        "timeframe",
        "date",
        "datetime",
    ]

    feature_cols = [col for col in df.columns if col not in exclude_cols]

    # ‡∏•‡∏ö columns ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô object
    for col in feature_cols[:]:
        if df[col].dtype == "object":
            feature_cols.remove(col)

    X = df[feature_cols].copy()
    y = df["target"].copy()

    # ‡∏•‡∏ö NaN
    mask = ~(X.isnull().any(axis=1) | y.isnull())
    X = X[mask]
    y = y[mask]

    print(f"üìä Features: {len(feature_cols)}")
    print(f"üìä Samples: {len(X):,}")
    print(f"üìä Target: UP={y.sum()}, DOWN={len(y) - y.sum()}")

    return X, y, feature_cols


def check_data_leakage(df):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Leakage"""
    print("\n" + "=" * 80)
    print("üîç ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Leakage")
    print("=" * 80)

    leakage_found = False

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö features ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏£‡∏±‡πà‡∏ß‡πÑ‡∏´‡∏•
    suspicious_cols = []
    for col in df.columns:
        if any(
            keyword in col.lower()
            for keyword in ["future", "next", "forward", "target"]
        ):
            if col != "target":
                suspicious_cols.append(col)

    if suspicious_cols:
        print(f"‚ö†Ô∏è  ‡∏û‡∏ö columns ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏á‡∏™‡∏±‡∏¢: {len(suspicious_cols)}")
        print(f"   (columns ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å features ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)")
        for col in suspicious_cols[:5]:  # ‡πÅ‡∏™‡∏î‡∏á 5 ‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å
            print(f"   - {col}")
        if len(suspicious_cols) > 5:
            print(f"   - ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(suspicious_cols) - 5} columns")
        # ‡πÑ‡∏°‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô leakage ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡πÅ‡∏•‡πâ‡∏ß
        leakage_found = False
    else:
        print("‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö columns ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏á‡∏™‡∏±‡∏¢")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö correlation ‡∏™‡∏π‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏Å‡∏±‡∏ö target
    if "target" in df.columns:
        print("\nüìä ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Correlation ‡∏Å‡∏±‡∏ö Target...")

        numeric_cols = df.select_dtypes(include=[np.number]).columns
        correlations = (
            df[numeric_cols].corrwith(df["target"]).abs().sort_values(ascending=False)
        )

        high_corr = correlations[correlations > 0.9]
        if len(high_corr) > 1:  # ‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö target ‡πÄ‡∏≠‡∏á
            print(f"‚ö†Ô∏è  ‡∏û‡∏ö features ‡∏ó‡∏µ‡πà‡∏°‡∏µ correlation ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô 0.9:")
            for col, corr in high_corr.items():
                if col != "target":
                    print(f"   - {col}: {corr:.4f}")
            leakage_found = True
        else:
            print("‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö correlation ‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥")

    if not leakage_found:
        print("\n‚úÖ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì Data Leakage")
    else:
        print("\n‚ö†Ô∏è  ‡∏≠‡∏≤‡∏à‡∏°‡∏µ Data Leakage - ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°")

    return leakage_found


def validate_lstm(model, scaler, X, y):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö LSTM model"""
    print("\n" + "=" * 80)
    print("üîç Validating LSTM Model")
    print("=" * 80)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    print(f"\nüìä Data Split:")
    print(f"   Train: {len(X_train):,} samples")
    print(f"   Test:  {len(X_test):,} samples")

    # Normalize
    if scaler is None:
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
    else:
        X_train_scaled = scaler.transform(X_train)

    X_test_scaled = scaler.transform(X_test)

    # Reshape for LSTM
    X_train_reshaped = X_train_scaled.reshape(
        (X_train_scaled.shape[0], 1, X_train_scaled.shape[1])
    )
    X_test_reshaped = X_test_scaled.reshape(
        (X_test_scaled.shape[0], 1, X_test_scaled.shape[1])
    )

    # Evaluate
    print("\nüìä Evaluating...")
    train_loss, train_acc = model.evaluate(X_train_reshaped, y_train, verbose=0)
    test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=0)

    print(f"\n‚úÖ Training Accuracy: {train_acc * 100:.2f}%")
    print(f"‚úÖ Test Accuracy:     {test_acc * 100:.2f}%")
    print(f"üìä Accuracy Gap:      {(train_acc - test_acc) * 100:.2f}%")

    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Overfitting
    gap = train_acc - test_acc
    if gap < 0.02:
        print("‚úÖ Model ‡πÑ‡∏°‡πà Overfit (gap < 2%)")
    elif gap < 0.05:
        print("‚ö†Ô∏è  Model Overfit ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (gap 2-5%)")
    elif gap < 0.10:
        print("‚ö†Ô∏è  Model Overfit ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (gap 5-10%)")
    else:
        print("‚ùå Model Overfit ‡∏°‡∏≤‡∏Å! (gap > 10%)")

    # Predictions
    y_pred_proba = model.predict(X_test_reshaped, verbose=0)
    y_pred = (y_pred_proba > 0.5).astype(int).flatten()

    # Confusion Matrix
    print("\nüìä Confusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    print(f"         Predicted")
    print(f"         DOWN    UP")
    print(f"Actual DOWN  {cm[0][0]:5d}  {cm[0][1]:5d}")
    print(f"       UP    {cm[1][0]:5d}  {cm[1][1]:5d}")

    # Additional metrics
    print("\nüìä Detailed Metrics:")
    print(classification_report(y_test, y_pred, target_names=["DOWN", "UP"], digits=4))

    # AUC-ROC
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"üìä AUC-ROC: {auc:.4f}")

    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
    print("\nüí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:")
    if test_acc > 0.95:
        print("   ‚ö†Ô∏è  Test Accuracy ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô 95% - ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Leakage")
    if gap > 0.10:
        print("   ‚ö†Ô∏è  Overfit ‡∏™‡∏π‡∏á - ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° Regularization ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î Model Complexity")
    if auc > 0.99:
        print("   ‚ö†Ô∏è  AUC-ROC ‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏ö‡∏ö - ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

    return {
        "train_acc": train_acc,
        "test_acc": test_acc,
        "gap": gap,
        "auc": auc,
        "overfitting": gap > 0.10,
    }


def validate_xgboost(model, scaler, X, y):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö XGBoost model"""
    print("\n" + "=" * 80)
    print("üîç Validating XGBoost Model")
    print("=" * 80)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    print(f"\nüìä Data Split:")
    print(f"   Train: {len(X_train):,} samples")
    print(f"   Test:  {len(X_test):,} samples")

    # Scale if scaler exists
    if scaler:
        X_train_scaled = scaler.transform(X_train)
        X_test_scaled = scaler.transform(X_test)
    else:
        X_train_scaled = X_train
        X_test_scaled = X_test

    # Evaluate
    train_pred = model.predict(X_train_scaled)
    test_pred = model.predict(X_test_scaled)

    train_acc = (train_pred == y_train).mean()
    test_acc = (test_pred == y_test).mean()

    print(f"\n‚úÖ Training Accuracy: {train_acc * 100:.2f}%")
    print(f"‚úÖ Test Accuracy:     {test_acc * 100:.2f}%")
    print(f"üìä Accuracy Gap:      {(train_acc - test_acc) * 100:.2f}%")

    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Overfitting
    gap = train_acc - test_acc
    if gap < 0.02:
        print("‚úÖ Model ‡πÑ‡∏°‡πà Overfit (gap < 2%)")
    elif gap < 0.05:
        print("‚ö†Ô∏è  Model Overfit ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (gap 2-5%)")
    elif gap < 0.10:
        print("‚ö†Ô∏è  Model Overfit ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (gap 5-10%)")
    else:
        print("‚ùå Model Overfit ‡∏°‡∏≤‡∏Å! (gap > 10%)")

    # Confusion Matrix
    print("\nüìä Confusion Matrix:")
    cm = confusion_matrix(y_test, test_pred)
    print(f"         Predicted")
    print(f"         DOWN    UP")
    print(f"Actual DOWN  {cm[0][0]:5d}  {cm[0][1]:5d}")
    print(f"       UP    {cm[1][0]:5d}  {cm[1][1]:5d}")

    # Additional metrics
    print("\nüìä Detailed Metrics:")
    print(
        classification_report(y_test, test_pred, target_names=["DOWN", "UP"], digits=4)
    )

    # AUC-ROC
    if hasattr(model, "predict_proba"):
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        auc = roc_auc_score(y_test, y_pred_proba)
        print(f"üìä AUC-ROC: {auc:.4f}")
    else:
        auc = None

    return {
        "train_acc": train_acc,
        "test_acc": test_acc,
        "gap": gap,
        "auc": auc,
        "overfitting": gap > 0.10,
    }


def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    parser = argparse.ArgumentParser(description="Model Validation")
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="Path to LSTM model (optional, ‡∏à‡∏∞‡πÉ‡∏ä‡πâ XGBoost ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏)",
    )
    parser.add_argument(
        "--data",
        type=str,
        default="data/processed/XAUUSD_M5_features_with_target_extended_target_threshold.csv",
        help="Path to data",
    )
    args = parser.parse_args()

    try:
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        X, y, feature_cols = load_data(args.data)
        if X is None:
            sys.exit(1)

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Leakage
        df = pd.read_csv(args.data)
        check_data_leakage(df)

        # Validate model
        if args.model:
            # LSTM Model
            model, scaler, model_features = load_lstm_model(args.model)
            if model is None:
                sys.exit(1)
            results = validate_lstm(model, scaler, X, y)
        else:
            # XGBoost Model
            model, scaler = load_xgboost_model()
            if model is None:
                print("‚ö†Ô∏è  ‡πÑ‡∏°‡πà‡∏û‡∏ö XGBoost model, ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏ --model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LSTM")
                sys.exit(1)
            results = validate_xgboost(model, scaler, X, y)

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        print("\n" + "=" * 80)
        print("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö")
        print("=" * 80)
        print(f"\n‚úÖ Training Accuracy: {results['train_acc'] * 100:.2f}%")
        print(f"‚úÖ Test Accuracy:     {results['test_acc'] * 100:.2f}%")
        print(f"üìä Gap:               {results['gap'] * 100:.2f}%")
        if results["auc"]:
            print(f"üìä AUC-ROC:           {results['auc']:.4f}")

        print("\nüí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢:")
        if results["overfitting"]:
            print("   ‚ùå Model ‡∏°‡∏µ Overfitting - ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á")
            print("   üí° ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢:")
            print("      - ‡πÄ‡∏û‡∏¥‡πà‡∏° Dropout")
            print("      - ‡πÄ‡∏û‡∏¥‡πà‡∏° Regularization")
            print("      - ‡∏•‡∏î Model Complexity")
            print("      - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        elif results["test_acc"] > 0.95:
            print("   ‚ö†Ô∏è  Accuracy ‡∏™‡∏π‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ - ‡∏Ñ‡∏ß‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Leakage ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
        elif results["test_acc"] > 0.80:
            print("   ‚úÖ Model ‡∏î‡∏µ‡∏°‡∏≤‡∏Å! ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Paper Trading ‡πÑ‡∏î‡πâ")
        elif results["test_acc"] > 0.70:
            print("   ‚úÖ Model ‡∏î‡∏µ ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á")
        else:
            print("   ‚ö†Ô∏è  Accuracy ‡∏ï‡πà‡∏≥ - ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Model")

        print("\n" + "=" * 80)
        print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
        print("=" * 80)

    except Exception as e:
        print(f"\n‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
