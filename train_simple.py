"""
Simple LSTM Training Script
============================
‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô LSTM model

‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ:
    python train_simple.py

‡∏´‡∏£‡∏∑‡∏≠:
    python train_simple.py --epochs 100 --batch_size 32
"""

import pandas as pd
import numpy as np
from pathlib import Path
import pickle
from datetime import datetime
import sys
import argparse

# Deep Learning
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

print("=" * 80)
print("[Launch] Simple LSTM Training Script")
print("=" * 80)


def load_data():
    """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà"""
    print("\nüìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")

    data_dir = Path("data/processed")

    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ
    possible_files = [
        "XAUUSD_M5_features_with_target_extended.csv",
        "XAUUSD_M15_features_with_target_extended.csv",
        "XAUUSD_H1_features_with_target_extended.csv",
        "XAUUSD_H4_features_with_target_extended.csv",
        "XAUUSD_M15_features_with_target.csv",
        "XAUUSD_H1_features_complete.csv",
        "XAUUSD_M5_features_complete.csv",
    ]

    df = None
    for filename in possible_files:
        filepath = data_dir / filename
        if filepath.exists():
            print(f"[OK] ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {filename}")
            df = pd.read_csv(filepath)
            break

    if df is None:
        # ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å daily_update
        recent_file = Path("data/processed_data_20251105.csv")
        if recent_file.exists():
            print(f"[OK] ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {recent_file.name}")
            df = pd.read_csv(recent_file)
        else:
            print("[Error] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•!")
            print("\n[Tip] ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏£‡∏±‡∏ô daily_update.bat ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô")
            sys.exit(1)

    print(f"[Stats] ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(df)} ‡πÅ‡∏ñ‡∏ß, {len(df.columns)} columns")
    return df


def prepare_data(df):
    """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô"""
    print("\n[Feature Engineering] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á target ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    if "target" not in df.columns:
        print("[Warning]  ‡πÑ‡∏°‡πà‡∏û‡∏ö target column, ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á...")
        df["future_price"] = df["close"].shift(-4)
        df["target"] = (df["future_price"] > df["close"]).astype(int)

    # ‡∏Å‡∏≥‡∏à‡∏±‡∏î columns ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    exclude_cols = [
        "target",
        "future_price",
        "time",
        "timestamp",
        "symbol",
        "timeframe",
        "date",
        "datetime",
    ]

    feature_cols = [col for col in df.columns if col not in exclude_cols]

    # ‡∏•‡∏ö columns ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô object ‡∏´‡∏£‡∏∑‡∏≠ string
    for col in feature_cols[:]:
        if df[col].dtype == "object":
            feature_cols.remove(col)
            print(f"   [Warning]  ‡∏Ç‡πâ‡∏≤‡∏°column: {col} (‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)")

    print(f"[Stats] ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features: {len(feature_cols)}")

    # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    X = df[feature_cols].copy()
    y = df["target"].copy()

    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN
    mask = ~(X.isnull().any(axis=1) | y.isnull())
    X = X[mask]
    y = y[mask]

    print(f"[Stats] ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î: {len(X)} ‡πÅ‡∏ñ‡∏ß")
    print(f"[Stats] Target distribution: UP={y.sum()}, DOWN={len(y) - y.sum()}")

    if len(X) < 100:
        print("[Error] ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠! (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 100 ‡πÅ‡∏ñ‡∏ß)")
        sys.exit(1)

    return X, y, feature_cols


def create_model(input_shape, units=64, dropout=0.3):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á LSTM model"""
    print("\nü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Model...")

    model = keras.Sequential(
        [
            layers.Input(shape=input_shape),
            layers.LSTM(units, return_sequences=True),
            layers.Dropout(dropout),
            layers.LSTM(units // 2),
            layers.Dropout(dropout),
            layers.Dense(32, activation="relu"),
            layers.Dropout(dropout / 2),
            layers.Dense(1, activation="sigmoid"),
        ]
    )

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss="binary_crossentropy",
        metrics=["accuracy"],
    )

    print("[OK] Model ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")
    print(f"[Stats] Parameters: {model.count_params():,}")

    return model


def train_model(X, y, epochs=50, batch_size=32, validation_split=0.2):
    """‡πÄ‡∏ó‡∏£‡∏ô model"""
    print("\n" + "=" * 80)
    print("[Target] ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô")
    print("=" * 80)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"[Stats] Training set: {len(X_train)} ‡πÅ‡∏ñ‡∏ß")
    print(f"[Stats] Test set: {len(X_test)} ‡πÅ‡∏ñ‡∏ß")

    # Normalize
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Reshape for LSTM (samples, timesteps, features)
    X_train_reshaped = X_train_scaled.reshape(
        (X_train_scaled.shape[0], 1, X_train_scaled.shape[1])
    )
    X_test_reshaped = X_test_scaled.reshape(
        (X_test_scaled.shape[0], 1, X_test_scaled.shape[1])
    )

    # Create model
    model = create_model(input_shape=(1, X_train_scaled.shape[1]))

    # Callbacks
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor="val_loss", patience=30, restore_best_weights=True, verbose=1
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss", factor=0.5, patience=10, verbose=1, min_lr=1e-7
        ),
    ]

    # Train
    print(f"\n[Reload] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô {epochs} epochs...\n")

    history = model.fit(
        X_train_reshaped,
        y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=validation_split,
        callbacks=callbacks,
        verbose=1,
    )

    # Evaluate
    print("\n" + "=" * 80)
    print("[Stats] ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•")
    print("=" * 80)

    train_loss, train_acc = model.evaluate(X_train_reshaped, y_train, verbose=0)
    test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=0)

    print(f"\n[OK] Training Accuracy: {train_acc * 100:.2f}%")
    print(f"[OK] Test Accuracy: {test_acc * 100:.2f}%")

    # Predictions
    y_pred_proba = model.predict(X_test_reshaped, verbose=0)
    y_pred = (y_pred_proba > 0.5).astype(int).flatten()

    print("\n[Stats] Classification Report:")
    print(classification_report(y_test, y_pred, target_names=["DOWN", "UP"]))

    print("\n[Stats] Confusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    print(f"         Predicted")
    print(f"         DOWN  UP")
    print(f"Actual DOWN  {cm[0][0]:4d}  {cm[0][1]:4d}")
    print(f"       UP    {cm[1][0]:4d}  {cm[1][1]:4d}")

    return (
        model,
        scaler,
        history,
        {
            "train_acc": train_acc,
            "test_acc": test_acc,
            "train_loss": train_loss,
            "test_loss": test_loss,
        },
    )


def save_model(model, scaler, feature_cols, metrics):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å model"""
    print("\n" + "=" * 80)
    print("[Save] ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Model")
    print("=" * 80)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
    models_dir = Path("models")
    models_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å model
    model_path = models_dir / f"lstm_simple_{timestamp}.keras"
    model.save(model_path)
    print(f"[OK] Model: {model_path}")

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scaler
    scaler_path = models_dir / f"scaler_simple_{timestamp}.pkl"
    with open(scaler_path, "wb") as f:
        pickle.dump(scaler, f)
    print(f"[OK] Scaler: {scaler_path}")

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å feature names
    features_path = models_dir / f"features_simple_{timestamp}.pkl"
    with open(features_path, "wb") as f:
        pickle.dump(feature_cols, f)
    print(f"[OK] Features: {features_path}")

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metadata
    metadata = {
        "timestamp": timestamp,
        "train_accuracy": float(metrics["train_acc"]),
        "test_accuracy": float(metrics["test_acc"]),
        "train_loss": float(metrics["train_loss"]),
        "test_loss": float(metrics["test_loss"]),
        "num_features": len(feature_cols),
        "model_file": str(model_path.name),
        "scaler_file": str(scaler_path.name),
        "features_file": str(features_path.name),
    }

    import json

    metadata_path = models_dir / f"metadata_simple_{timestamp}.json"
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=4)
    print(f"[OK] Metadata: {metadata_path}")

    print("\n" + "=" * 80)
    print("[OK] ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
    print("=" * 80)

    return model_path


def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    # Parse arguments
    parser = argparse.ArgumentParser(description="Simple LSTM Training")
    parser.add_argument("--data", type=str, default=None, help="‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (optional)")
    parser.add_argument(
        "--epochs", type=int, default=100, help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epochs (default: 100)"
    )
    parser.add_argument(
        "--batch_size", type=int, default=32, help="Batch size (default: 32)"
    )
    parser.add_argument(
        "--units", type=int, default=64, help="LSTM units (default: 64)"
    )
    args = parser.parse_args()

    try:
        # Load data
        if args.data:
            print(f"üìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å: {args.data}")
            df = pd.read_csv(args.data)
            print(f"[Stats] ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(df)} ‡πÅ‡∏ñ‡∏ß, {len(df.columns)} columns")
        else:
            df = load_data()

        # Prepare data
        X, y, feature_cols = prepare_data(df)

        # Train model
        model, scaler, history, metrics = train_model(
            X, y, epochs=args.epochs, batch_size=args.batch_size
        )

        # Save model
        model_path = save_model(model, scaler, feature_cols, metrics)

        # Summary
        print("\n" + "[Success]" * 40)
        print("\n[OK] ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!\n")
        print(f"[Stats] Test Accuracy: {metrics['test_acc'] * 100:.2f}%")
        print(f"[Save] Model saved: {model_path.name}")
        print("\n[Tip] ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ:")
        print("   1. ‡∏£‡∏±‡∏ô daily_update.bat ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
        print("   2. ‡∏£‡∏±‡∏ô paper_trading.py ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏ó‡∏£‡∏î")
        print("   3. ‡∏£‡∏±‡∏ô live_trading.py ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏£‡∏î‡∏à‡∏£‡∏¥‡∏á")
        print("\n" + "[Success]" * 40 + "\n")

    except Exception as e:
        print(f"\n[Error] ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
